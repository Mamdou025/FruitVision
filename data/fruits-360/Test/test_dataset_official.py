#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test du mod√®le FruitVision sur le dataset officiel Fruits-360
Objectif: V√©rifier les performances sur donn√©es du m√™me environnement
"""

import tensorflow as tf
import numpy as np
from PIL import Image
import os
import glob
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def load_model_and_classes():
    """Charger le mod√®le et les classes"""
    try:
        model = tf.keras.models.load_model('models/fruivision_split_manuel.h5')
        classes = ['Pomme', 'Banane', 'Kiwi', 'Citron', 'Peche']
        print("‚úÖ Mod√®le charg√© avec succ√®s")
        print(f"‚úÖ Classes: {classes}")
        return model, classes
    except:
        print("‚ùå Erreur: Impossible de charger le mod√®le")
        return None, None

def preprocess_image(image_path, target_size=(100, 100)):
    """Pr√©processer une image comme pendant l'entra√Ænement"""
    try:
        image = Image.open(image_path).convert('RGB')
        image = image.resize(target_size)
        image_array = np.array(image) / 255.0
        image_array = np.expand_dims(image_array, axis=0)
        return image_array
    except Exception as e:
        print(f"‚ùå Erreur avec {image_path}: {e}")
        return None

def test_on_official_dataset(model, classes, test_dir="Test"):
    """Tester sur le dataset officiel Fruits-360"""
    
    print("\n" + "="*60)
    print("üß™ TEST SUR DATASET OFFICIEL FRUITS-360")
    print("="*60)
    
    all_predictions = []
    all_true_labels = []
    all_confidences = []
    results_by_class = {}
    
    # Mapper les noms de classes du dataset vers nos classes
    class_mapping = {
        'Apple Red 1': 'Pomme',
        'Apple Red 2': 'Pomme', 
        'Apple Red 3': 'Pomme',
        'Apple Red Delicious': 'Pomme',
        'Banana': 'Banane',
        'Banana Red': 'Banane',
        'Kiwi': 'Kiwi',
        'Lemon': 'Citron',
        'Lemon Meyer': 'Citron',
        'Peach': 'Peche',
        'Peach 2': 'Peche'
    }
    
    if not os.path.exists(test_dir):
        print(f"‚ùå Dossier {test_dir} introuvable")
        return
    
    # Parcourir chaque classe dans le dossier Test
    for class_folder in os.listdir(test_dir):
        class_path = os.path.join(test_dir, class_folder)
        
        if not os.path.isdir(class_path):
            continue
            
        # V√©rifier si cette classe nous int√©resse
        if class_folder not in class_mapping:
            continue
            
        true_class = class_mapping[class_folder]
        
        print(f"\nüçé Analyse de {class_folder} -> {true_class}:")
        
        class_predictions = []
        class_confidences = []
        correct_predictions = 0
        total_images = 0
        
        # Tester toutes les images de cette classe
        image_files = glob.glob(os.path.join(class_path, "*.jpg"))
        
        for image_path in image_files[:20]:  # Limiter √† 20 images par classe
            image_array = preprocess_image(image_path)
            
            if image_array is None:
                continue
                
            # Pr√©diction
            prediction = model.predict(image_array, verbose=0)
            pred_idx = np.argmax(prediction[0])
            confidence = prediction[0][pred_idx]
            predicted_class = classes[pred_idx]
            
            # Stocker les r√©sultats
            all_predictions.append(predicted_class)
            all_true_labels.append(true_class)
            all_confidences.append(confidence)
            
            class_predictions.append(predicted_class)
            class_confidences.append(confidence)
            
            if predicted_class == true_class:
                correct_predictions += 1
                
            total_images += 1
            
            # Afficher quelques exemples
            if total_images <= 5:
                status = "‚úÖ" if predicted_class == true_class else "‚ùå"
                filename = os.path.basename(image_path)
                print(f"   {filename:<15} ‚Üí {predicted_class:<8} ({confidence:.1%}) {status}")
        
        # Statistiques par classe
        if total_images > 0:
            class_accuracy = correct_predictions / total_images
            avg_confidence = np.mean(class_confidences)
            
            results_by_class[true_class] = {
                'accuracy': class_accuracy,
                'confidence': avg_confidence,
                'total': total_images,
                'correct': correct_predictions
            }
            
            print(f"   üìä Accuracy: {class_accuracy:.1%} ({correct_predictions}/{total_images})")
            print(f"   üìä Confiance moyenne: {avg_confidence:.1%}")
    
    # Statistiques globales
    print("\n" + "="*60)
    print("üìä R√âSULTATS GLOBAUX")
    print("="*60)
    
    if all_predictions:
        overall_accuracy = accuracy_score(all_true_labels, all_predictions)
        avg_confidence = np.mean(all_confidences)
        
        print(f"üéØ Accuracy globale: {overall_accuracy:.1%}")
        print(f"üéØ Confiance moyenne: {avg_confidence:.1%}")
        print(f"üéØ Total d'images test√©es: {len(all_predictions)}")
        
        # D√©tail par classe
        print(f"\nüìã D√âTAIL PAR CLASSE:")
        for class_name, stats in results_by_class.items():
            print(f"   {class_name:<8}: {stats['accuracy']:.1%} ({stats['correct']}/{stats['total']})")
        
        # Matrice de confusion
        print(f"\nüîç MATRICE DE CONFUSION:")
        unique_classes = list(set(all_true_labels))
        cm = confusion_matrix(all_true_labels, all_predictions, labels=unique_classes)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=unique_classes, yticklabels=unique_classes)
        plt.title('Matrice de Confusion - Dataset Officiel')
        plt.xlabel('Pr√©dictions')
        plt.ylabel('Vraies Classes')
        plt.tight_layout()
        plt.show()
        
        # Rapport de classification
        print(f"\nüìù RAPPORT D√âTAILL√â:")
        print(classification_report(all_true_labels, all_predictions))
        
        return overall_accuracy
    else:
        print("‚ùå Aucune image trouv√©e √† tester")
        return 0

def main():
    """Fonction principale"""
    print("üîç TEST SUR DATASET OFFICIEL FRUITS-360")
    print("Objectif: V√©rifier les performances sur le m√™me environnement")
    print("="*70)
    
    # Charger le mod√®le
    model, classes = load_model_and_classes()
    if model is None:
        return
    
    # Tester sur le dataset officiel
    accuracy = test_on_official_dataset(model, classes)
    
    print(f"\nüéØ CONCLUSION:")
    if accuracy > 0.95:
        print(f"‚úÖ Excellente performance: {accuracy:.1%}")
        print("‚úÖ Le mod√®le fonctionne parfaitement sur le dataset officiel")
        print("üí° Ceci confirme que le probl√®me vient de la g√©n√©ralisation")
        print("üí° Le dataset Fruits-360 est trop artificiel pour le monde r√©el")
    elif accuracy > 0.80:
        print(f"‚ö†Ô∏è  Performance correcte: {accuracy:.1%}")
        print("‚ö†Ô∏è  Il pourrait y avoir des probl√®mes de pr√©processing")
    else:
        print(f"‚ùå Performance faible: {accuracy:.1%}")
        print("‚ùå Il y a un probl√®me avec le mod√®le ou les donn√©es")

if __name__ == "__main__":
    main()