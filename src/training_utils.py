"""
FruitVision - Utilitaires d'EntraÃ®nement
========================================

Ce module contient des fonctions utilitaires pour l'entraÃ®nement, l'Ã©valuation et la visualisation
des performances du modÃ¨le CNN.

Auteur: Mamadou Fall
Date: 2025
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import pandas as pd


class GestionnaireEntrainement:
    """
    Classe pour gÃ©rer l'entraÃ®nement et l'Ã©valuation du modÃ¨le.
    
    Cette classe centralise:
    - La configuration des callbacks
    - La sauvegarde de l'historique
    - L'Ã©valuation des performances
    - La gÃ©nÃ©ration de visualisations
    """
    
    def __init__(self, dossier_resultats: str = "results"):
        """
        Initialiser le gestionnaire d'entraÃ®nement.
        
        Args:
            dossier_resultats (str): Dossier pour sauvegarder les rÃ©sultats
        """
        self.dossier_resultats = dossier_resultats
        self.historique = None
        self.modele = None
        self.noms_classes = None
        
        # CrÃ©er le dossier de rÃ©sultats
        os.makedirs(dossier_resultats, exist_ok=True)
        
        print(f"ğŸ“ Gestionnaire d'entraÃ®nement initialisÃ©")
        print(f"   Dossier de rÃ©sultats: {dossier_resultats}")
    
    
    def configurer_callbacks(self, config_arret_precoce: Dict, config_reduction_lr: Dict):
        """
        Configurer les callbacks pour l'entraÃ®nement.
        
        Pourquoi ces callbacks?
        - EarlyStopping: Ã‰vite le surapprentissage en arrÃªtant quand la performance stagne
        - ReduceLROnPlateau: RÃ©duit le taux d'apprentissage pour affiner l'entraÃ®nement
        - ModelCheckpoint: Sauvegarde les meilleurs modÃ¨les
        
        Args:
            config_arret_precoce (Dict): Configuration pour l'arrÃªt prÃ©coce
            config_reduction_lr (Dict): Configuration pour la rÃ©duction du LR
            
        Returns:
            List: Liste des callbacks configurÃ©s
        """
        try:
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
        except ImportError:
            print("âš ï¸ TensorFlow non disponible - callbacks simulÃ©s")
            return []
        
        callbacks = []
        
        # 1. ARRÃŠT PRÃ‰COCE (Early Stopping)
        early_stopping = EarlyStopping(
            monitor=config_arret_precoce['surveiller'],
            patience=config_arret_precoce['patience'],
            min_delta=config_arret_precoce['delta_min'],
            mode=config_arret_precoce['mode'],
            restore_best_weights=config_arret_precoce['restaurer_meilleurs_poids'],
            verbose=config_arret_precoce['verbose']
        )
        callbacks.append(early_stopping)
        
        # 2. RÃ‰DUCTION DU TAUX D'APPRENTISSAGE
        reduce_lr = ReduceLROnPlateau(
            monitor=config_reduction_lr['surveiller'],
            factor=config_reduction_lr['facteur'],
            patience=config_reduction_lr['patience'],
            min_lr=config_reduction_lr['lr_min'],
            verbose=config_reduction_lr['verbose']
        )
        callbacks.append(reduce_lr)
        
        # 3. SAUVEGARDE DU MEILLEUR MODÃˆLE
        chemin_checkpoint = os.path.join(self.dossier_resultats, 'meilleur_modele.h5')
        model_checkpoint = ModelCheckpoint(
            filepath=chemin_checkpoint,
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1
        )
        callbacks.append(model_checkpoint)
        
        print("âœ… Callbacks configurÃ©s:")
        print(f"   ğŸ›‘ ArrÃªt prÃ©coce: patience={config_arret_precoce['patience']}")
        print(f"   ğŸ“‰ RÃ©duction LR: facteur={config_reduction_lr['facteur']}")
        print(f"   ğŸ’¾ Sauvegarde: {chemin_checkpoint}")
        
        return callbacks
    
    
    def sauvegarder_historique(self, historique, nom_fichier: str = "historique_entrainement.json"):
        """
        Sauvegarder l'historique d'entraÃ®nement.
        
        Args:
            historique: Historique retournÃ© par model.fit()
            nom_fichier (str): Nom du fichier de sauvegarde
        """
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        
        # Convertir l'historique en format JSON sÃ©rialisable
        historique_dict = {}
        for cle, valeurs in historique.history.items():
            # Convertir les valeurs numpy en listes Python
            historique_dict[cle] = [float(v) for v in valeurs]
        
        # Ajouter des mÃ©tadonnÃ©es
        historique_dict['metadata'] = {
            'date_entrainement': datetime.now().isoformat(),
            'nb_epoques_completees': len(historique_dict['loss']),
            'meilleure_val_accuracy': max(historique_dict.get('val_accuracy', [0])),
            'epoque_meilleure_accuracy': historique_dict.get('val_accuracy', [0]).index(
                max(historique_dict.get('val_accuracy', [0]))
            ) + 1 if 'val_accuracy' in historique_dict else 0
        }
        
        # Sauvegarder en JSON
        with open(chemin_complet, 'w', encoding='utf-8') as f:
            json.dump(historique_dict, f, indent=2, ensure_ascii=False)
        
        self.historique = historique_dict
        print(f"ğŸ“Š Historique sauvegardÃ©: {chemin_complet}")
        print(f"   ğŸ† Meilleure val_accuracy: {historique_dict['metadata']['meilleure_val_accuracy']:.4f}")
    
    
    def tracer_courbes_apprentissage(self, historique=None, nom_fichier: str = "courbes_apprentissage.png"):
        """
        CrÃ©er et sauvegarder les courbes d'apprentissage.
        
        Args:
            historique: Historique d'entraÃ®nement (utilise self.historique si None)
            nom_fichier (str): Nom du fichier image
        """
        if historique is None:
            historique = self.historique
        
        if historique is None:
            print("âŒ Pas d'historique disponible pour tracer les courbes")
            return
        
        # Configuration du graphique
        plt.style.use('default')
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Courbe 1: Accuracy (PrÃ©cision)
        epoques = range(1, len(historique['accuracy']) + 1)
        
        ax1.plot(epoques, historique['accuracy'], 'b-', label='PrÃ©cision EntraÃ®nement', linewidth=2)
        if 'val_accuracy' in historique:
            ax1.plot(epoques, historique['val_accuracy'], 'r-', label='PrÃ©cision Validation', linewidth=2)
        
        ax1.set_title('Ã‰volution de la PrÃ©cision', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Ã‰poque')
        ax1.set_ylabel('PrÃ©cision')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1])
        
        # Courbe 2: Loss (Perte)
        ax2.plot(epoques, historique['loss'], 'b-', label='Perte EntraÃ®nement', linewidth=2)
        if 'val_loss' in historique:
            ax2.plot(epoques, historique['val_loss'], 'r-', label='Perte Validation', linewidth=2)
        
        ax2.set_title('Ã‰volution de la Perte', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Ã‰poque')
        ax2.set_ylabel('Perte')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Ajouter des annotations pour les meilleurs rÃ©sultats
        if 'val_accuracy' in historique:
            meilleure_val_acc = max(historique['val_accuracy'])
            meilleure_epoque = historique['val_accuracy'].index(meilleure_val_acc) + 1
            
            ax1.annotate(f'Meilleur: {meilleure_val_acc:.3f}\n(Ã‰poque {meilleure_epoque})',
                        xy=(meilleure_epoque, meilleure_val_acc),
                        xytext=(meilleure_epoque + len(epoques)*0.1, meilleure_val_acc),
                        arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),
                        bbox=dict(boxstyle="round,pad=0.3", facecolor='yellow', alpha=0.7),
                        fontsize=10)
        
        plt.tight_layout()
        
        # Sauvegarder
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        plt.savefig(chemin_complet, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“ˆ Courbes d'apprentissage sauvegardÃ©es: {chemin_complet}")
    
    
    def evaluer_modele(self, modele, X_test: np.ndarray, y_test: np.ndarray, 
                      noms_classes: List[str]) -> Dict:
        """
        Ã‰valuer le modÃ¨le sur l'ensemble de test.
        
        Args:
            modele: ModÃ¨le entraÃ®nÃ©
            X_test (np.ndarray): Images de test
            y_test (np.ndarray): Labels de test (one-hot)
            noms_classes (List[str]): Noms des classes
            
        Returns:
            Dict: MÃ©triques d'Ã©valuation
        """
        print("ğŸ§ª Ã‰valuation du modÃ¨le sur l'ensemble de test...")
        
        # PrÃ©dictions
        y_pred_proba = modele.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        y_true = np.argmax(y_test, axis=1)
        
        # MÃ©triques globales
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average='weighted'
        )
        
        # MÃ©triques par classe
        rapport_classification = classification_report(
            y_true, y_pred, target_names=noms_classes, output_dict=True
        )
        
        # PrÃ©parer les rÃ©sultats
        resultats = {
            'accuracy_globale': float(accuracy),
            'precision_moyenne': float(precision),
            'recall_moyen': float(recall),
            'f1_score_moyen': float(f1),
            'rapport_par_classe': rapport_classification,
            'nb_echantillons_test': len(y_test),
            'predictions_correctes': int(np.sum(y_pred == y_true)),
            'predictions_incorrectes': int(np.sum(y_pred != y_true))
        }
        
        # Sauvegarder les mÃ©triques
        self.noms_classes = noms_classes
        self._sauvegarder_metriques(resultats)
        
        # Afficher un rÃ©sumÃ©
        print(f"âœ… Ã‰valuation terminÃ©e:")
        print(f"   ğŸ¯ PrÃ©cision globale: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"   ğŸ“Š F1-Score moyen: {f1:.4f}")
        print(f"   âœ… PrÃ©dictions correctes: {resultats['predictions_correctes']}/{resultats['nb_echantillons_test']}")
        
        return resultats
    
    
    def creer_matrice_confusion(self, y_true: np.ndarray, y_pred: np.ndarray, 
                               noms_classes: List[str], nom_fichier: str = "matrice_confusion.png"):
        """
        CrÃ©er et sauvegarder la matrice de confusion.
        
        Args:
            y_true (np.ndarray): Vraies Ã©tiquettes
            y_pred (np.ndarray): Ã‰tiquettes prÃ©dites
            noms_classes (List[str]): Noms des classes
            nom_fichier (str): Nom du fichier de sauvegarde
        """
        # Calculer la matrice de confusion
        cm = confusion_matrix(y_true, y_pred)
        
        # CrÃ©er le graphique
        plt.figure(figsize=(10, 8))
        
        # Utiliser seaborn pour une matrice plus jolie
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=noms_classes, yticklabels=noms_classes,
                   square=True, linewidths=0.5)
        
        plt.title('Matrice de Confusion - FruitVision', fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('PrÃ©dictions du ModÃ¨le', fontsize=12)
        plt.ylabel('Vraies Ã‰tiquettes', fontsize=12)
        
        # Rotation des labels pour meilleure lisibilitÃ©
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        
        # Sauvegarder
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        plt.savefig(chemin_complet, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"ğŸ“Š Matrice de confusion sauvegardÃ©e: {chemin_complet}")
        
        # Analyser les erreurs communes
        self._analyser_erreurs_confusion(cm, noms_classes)
    
    
    def _analyser_erreurs_confusion(self, cm: np.ndarray, noms_classes: List[str]):
        """
        Analyser les erreurs les plus communes dans la matrice de confusion.
        
        Args:
            cm (np.ndarray): Matrice de confusion
            noms_classes (List[str]): Noms des classes
        """
        print("\nğŸ” Analyse des erreurs les plus frÃ©quentes:")
        
        # Trouver les erreurs (Ã©lÃ©ments hors diagonale)
        erreurs = []
        for i in range(len(noms_classes)):
            for j in range(len(noms_classes)):
                if i != j and cm[i, j] > 0:  # Erreur (pas sur la diagonale)
                    erreurs.append((cm[i, j], noms_classes[i], noms_classes[j]))
        
        # Trier par nombre d'erreurs dÃ©croissant
        erreurs.sort(reverse=True)
        
        # Afficher les 3 erreurs les plus frÃ©quentes
        print("   Top 3 des confusions:")
        for i, (nb_erreurs, vraie_classe, classe_predite) in enumerate(erreurs[:3]):
            print(f"   {i+1}. {vraie_classe} â†’ {classe_predite}: {nb_erreurs} erreurs")
        
        if not erreurs:
            print("   ğŸ‰ Aucune erreur dÃ©tectÃ©e! ModÃ¨le parfait!")
    
    
    def _sauvegarder_metriques(self, resultats: Dict, nom_fichier: str = "metriques_evaluation.json"):
        """
        Sauvegarder les mÃ©triques d'Ã©valuation.
        
        Args:
            resultats (Dict): RÃ©sultats de l'Ã©valuation
            nom_fichier (str): Nom du fichier
        """
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        
        # Ajouter timestamp
        resultats['metadata'] = {
            'date_evaluation': datetime.now().isoformat(),
            'modele_utilise': 'FruitVision CNN'
        }
        
        with open(chemin_complet, 'w', encoding='utf-8') as f:
            json.dump(resultats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š MÃ©triques sauvegardÃ©es: {chemin_complet}")


# Fonctions utilitaires indÃ©pendantes
def calculer_temps_entrainement(debut: datetime, fin: datetime) -> str:
    """
    Calculer et formater le temps d'entraÃ®nement.
    
    Args:
        debut (datetime): Heure de dÃ©but
        fin (datetime): Heure de fin
        
    Returns:
        str: Temps formatÃ©
    """
    duree = fin - debut
    heures = duree.seconds // 3600
    minutes = (duree.seconds % 3600) // 60
    secondes = duree.seconds % 60
    
    if heures > 0:
        return f"{heures}h {minutes}m {secondes}s"
    elif minutes > 0:
        return f"{minutes}m {secondes}s"
    else:
        return f"{secondes}s"


def afficher_resume_modele(modele, noms_classes: List[str]):
    """
    Afficher un rÃ©sumÃ© du modÃ¨le avec des informations utiles.
    
    Args:
        modele: ModÃ¨le Keras
        noms_classes (List[str]): Noms des classes
    """
    print("ğŸ§  RÃ©sumÃ© du ModÃ¨le FruitVision")
    print("=" * 50)
    
    # Informations gÃ©nÃ©rales
    try:
        total_params = modele.count_params()
        print(f"ğŸ“Š ParamÃ¨tres totaux: {total_params:,}")
        print(f"ğŸ’¾ Taille estimÃ©e: ~{total_params * 4 / 1024 / 1024:.1f} MB")
    except:
        print("ğŸ“Š Informations sur les paramÃ¨tres non disponibles")
    
    print(f"ğŸ Classes Ã  reconnaÃ®tre: {len(noms_classes)}")
    for i, nom in enumerate(noms_classes):
        print(f"   {i}: {nom}")
    
    print(f"ğŸ“ Forme d'entrÃ©e: {modele.input_shape}")
    print(f"ğŸ“¤ Forme de sortie: {model.output_shape}")


# Test du module
if __name__ == "__main__":
    """
    Code de test pour les utilitaires d'entraÃ®nement.
    """
    print("ğŸ§ª Test des Utilitaires d'EntraÃ®nement")
    print("=" * 50)
    
    # Test du gestionnaire
    gestionnaire = GestionnaireEntrainement("test_results")
    
    # Test avec des donnÃ©es factices
    print("\nğŸ”¬ Test avec donnÃ©es factices...")
    
    # Simuler un historique d'entraÃ®nement
    historique_factice = {
        'accuracy': [0.6, 0.7, 0.8, 0.85, 0.9],
        'val_accuracy': [0.5, 0.6, 0.75, 0.8, 0.82],
        'loss': [1.5, 1.2, 0.8, 0.5, 0.3],
        'val_loss': [1.7, 1.3, 0.9, 0.6, 0.4]
    }
    
    gestionnaire.historique = historique_factice
    
    # Test des courbes d'apprentissage
    print("ğŸ“ˆ Test des courbes d'apprentissage...")
    gestionnaire.tracer_courbes_apprentissage()
    
    # Test de la matrice de confusion avec donnÃ©es factices
    print("ğŸ“Š Test de la matrice de confusion...")
    y_true_factice = np.array([0, 1, 2, 0, 1, 2, 0, 1])
    y_pred_factice = np.array([0, 1, 1, 0, 2, 2, 1, 1])
    noms_classes_test = ['Pomme', 'Banane', 'Kiwi']
    
    gestionnaire.creer_matrice_confusion(y_true_factice, y_pred_factice, noms_classes_test)
    
    print("\nâœ… Tous les tests des utilitaires passÃ©s!")
    print("ğŸ“Œ PrÃªt pour l'entraÃ®nement rÃ©el du modÃ¨le")