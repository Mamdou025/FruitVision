"""
FruitVision - Utilitaires d'Entra√Ænement
========================================

Ce module contient des fonctions utilitaires pour l'entra√Ænement, l'√©valuation et la visualisation
des performances du mod√®le CNN.

Auteur: Mamadou Fall
Date: 2025
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import json
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import pandas as pd


class GestionnaireEntrainement:
    """
    Classe pour g√©rer l'entra√Ænement et l'√©valuation du mod√®le.
    
    Cette classe centralise:
    - La configuration des callbacks
    - La sauvegarde de l'historique
    - L'√©valuation des performances
    - La g√©n√©ration de visualisations
    """
    
    def __init__(self, dossier_resultats: str = "results"):
        """
        Initialiser le gestionnaire d'entra√Ænement.
        
        Args:
            dossier_resultats (str): Dossier pour sauvegarder les r√©sultats
        """
        self.dossier_resultats = dossier_resultats
        self.historique = None
        self.modele = None
        self.noms_classes = None
        
        # Cr√©er le dossier de r√©sultats
        os.makedirs(dossier_resultats, exist_ok=True)
        
        print(f"üìÅ Gestionnaire d'entra√Ænement initialis√©")
        print(f"   Dossier de r√©sultats: {dossier_resultats}")
    
    
    def configurer_callbacks(self, config_arret_precoce: Dict, config_reduction_lr: Dict):
        """
        Configurer les callbacks pour l'entra√Ænement.
        
        Pourquoi ces callbacks?
        - EarlyStopping: √âvite le surapprentissage en arr√™tant quand la performance stagne
        - ReduceLROnPlateau: R√©duit le taux d'apprentissage pour affiner l'entra√Ænement
        - ModelCheckpoint: Sauvegarde les meilleurs mod√®les
        
        Args:
            config_arret_precoce (Dict): Configuration pour l'arr√™t pr√©coce
            config_reduction_lr (Dict): Configuration pour la r√©duction du LR
            
        Returns:
            List: Liste des callbacks configur√©s
        """
        try:
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
        except ImportError:
            print("‚ö†Ô∏è TensorFlow non disponible - callbacks simul√©s")
            return []
        
        callbacks = []
        
        # 1. ARR√äT PR√âCOCE (Early Stopping)
        early_stopping = EarlyStopping(
            monitor=config_arret_precoce['surveiller'],
            patience=config_arret_precoce['patience'],
            min_delta=config_arret_precoce['delta_min'],
            mode=config_arret_precoce['mode'],
            restore_best_weights=config_arret_precoce['restaurer_meilleurs_poids'],
            verbose=config_arret_precoce['verbose']
        )
        callbacks.append(early_stopping)
        
        # 2. R√âDUCTION DU TAUX D'APPRENTISSAGE
        reduce_lr = ReduceLROnPlateau(
            monitor=config_reduction_lr['surveiller'],
            factor=config_reduction_lr['facteur'],
            patience=config_reduction_lr['patience'],
            min_lr=config_reduction_lr['lr_min'],
            verbose=config_reduction_lr['verbose']
        )
        callbacks.append(reduce_lr)
        
        # 3. SAUVEGARDE DU MEILLEUR MOD√àLE
        chemin_checkpoint = os.path.join(self.dossier_resultats, 'meilleur_modele.h5')
        model_checkpoint = ModelCheckpoint(
            filepath=chemin_checkpoint,
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1
        )
        callbacks.append(model_checkpoint)
        
        print("‚úÖ Callbacks configur√©s:")
        print(f"   üõë Arr√™t pr√©coce: patience={config_arret_precoce['patience']}")
        print(f"   üìâ R√©duction LR: facteur={config_reduction_lr['facteur']}")
        print(f"   üíæ Sauvegarde: {chemin_checkpoint}")
        
        return callbacks
    
    
    def sauvegarder_historique(self, historique, nom_fichier: str = "historique_entrainement.json"):
        """
        Sauvegarder l'historique d'entra√Ænement.
        
        Args:
            historique: Historique retourn√© par model.fit()
            nom_fichier (str): Nom du fichier de sauvegarde
        """
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        
        # Convertir l'historique en format JSON s√©rialisable
        historique_dict = {}
        for cle, valeurs in historique.history.items():
            # Convertir les valeurs numpy en listes Python
            historique_dict[cle] = [float(v) for v in valeurs]
        
        # Ajouter des m√©tadonn√©es
        historique_dict['metadata'] = {
            'date_entrainement': datetime.now().isoformat(),
            'nb_epoques_completees': len(historique_dict['loss']),
            'meilleure_val_accuracy': max(historique_dict.get('val_accuracy', [0])),
            'epoque_meilleure_accuracy': historique_dict.get('val_accuracy', [0]).index(
                max(historique_dict.get('val_accuracy', [0]))
            ) + 1 if 'val_accuracy' in historique_dict else 0
        }
        
        # Sauvegarder en JSON
        with open(chemin_complet, 'w', encoding='utf-8') as f:
            json.dump(historique_dict, f, indent=2, ensure_ascii=False)
        
        self.historique = historique_dict
        print(f"üìä Historique sauvegard√©: {chemin_complet}")
        print(f"   üèÜ Meilleure val_accuracy: {historique_dict['metadata']['meilleure_val_accuracy']:.4f}")
    
    
    def tracer_courbes_apprentissage(self, historique=None, nom_fichier: str = "courbes_apprentissage.png"):
        """
        Cr√©er et sauvegarder les courbes d'apprentissage.
        
        Args:
            historique: Historique d'entra√Ænement (utilise self.historique si None)
            nom_fichier (str): Nom du fichier image
        """
        if historique is None:
            historique = self.historique
        
        if historique is None:
            print("‚ùå Pas d'historique disponible pour tracer les courbes")
            return
        
        # Configuration du graphique
        plt.style.use('default')
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        
        # Courbe 1: Accuracy (Pr√©cision)
        epoques = range(1, len(historique['accuracy']) + 1)
        
        ax1.plot(epoques, historique['accuracy'], 'b-', label='Pr√©cision Entra√Ænement', linewidth=2)
        if 'val_accuracy' in historique:
            ax1.plot(epoques, historique['val_accuracy'], 'r-', label='Pr√©cision Validation', linewidth=2)
        
        ax1.set_title('√âvolution de la Pr√©cision', fontsize=14, fontweight='bold')
        ax1.set_xlabel('√âpoque')
        ax1.set_ylabel('Pr√©cision')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1])
        
        # Courbe 2: Loss (Perte)
        ax2.plot(epoques, historique['loss'], 'b-', label='Perte Entra√Ænement', linewidth=2)
        if 'val_loss' in historique:
            ax2.plot(epoques, historique['val_loss'], 'r-', label='Perte Validation', linewidth=2)
        
        ax2.set_title('√âvolution de la Perte', fontsize=14, fontweight='bold')
        ax2.set_xlabel('√âpoque')
        ax2.set_ylabel('Perte')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Ajouter des annotations pour les meilleurs r√©sultats
        if 'val_accuracy' in historique:
            meilleure_val_acc = max(historique['val_accuracy'])
            meilleure_epoque = historique['val_accuracy'].index(meilleure_val_acc) + 1
            
            ax1.annotate(f'Meilleur: {meilleure_val_acc:.3f}\n(√âpoque {meilleure_epoque})',
                        xy=(meilleure_epoque, meilleure_val_acc),
                        xytext=(meilleure_epoque + len(epoques)*0.1, meilleure_val_acc),
                        arrowprops=dict(arrowstyle='->', color='red', alpha=0.7),
                        bbox=dict(boxstyle="round,pad=0.3", facecolor='yellow', alpha=0.7),
                        fontsize=10)
        
        plt.tight_layout()
        
        # Sauvegarder
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        plt.savefig(chemin_complet, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"üìà Courbes d'apprentissage sauvegard√©es: {chemin_complet}")
    
    
    def evaluer_modele(self, modele, X_test: np.ndarray, y_test: np.ndarray, 
                      noms_classes: List[str]) -> Dict:
        """
        √âvaluer le mod√®le sur l'ensemble de test.
        
        Args:
            modele: Mod√®le entra√Æn√©
            X_test (np.ndarray): Images de test
            y_test (np.ndarray): Labels de test (one-hot)
            noms_classes (List[str]): Noms des classes
            
        Returns:
            Dict: M√©triques d'√©valuation
        """
        print("üß™ √âvaluation du mod√®le sur l'ensemble de test...")
        
        # Pr√©dictions
        y_pred_proba = modele.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_proba, axis=1)
        y_true = np.argmax(y_test, axis=1)
        
        # M√©triques globales
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average='weighted'
        )
        
        # M√©triques par classe
        rapport_classification = classification_report(
            y_true, y_pred, target_names=noms_classes, output_dict=True
        )
        
        # Pr√©parer les r√©sultats
        resultats = {
            'accuracy_globale': float(accuracy),
            'precision_moyenne': float(precision),
            'recall_moyen': float(recall),
            'f1_score_moyen': float(f1),
            'rapport_par_classe': rapport_classification,
            'nb_echantillons_test': len(y_test),
            'predictions_correctes': int(np.sum(y_pred == y_true)),
            'predictions_incorrectes': int(np.sum(y_pred != y_true))
        }
        
        # Sauvegarder les m√©triques
        self.noms_classes = noms_classes
        self._sauvegarder_metriques(resultats)
        
        # Afficher un r√©sum√©
        print(f"‚úÖ √âvaluation termin√©e:")
        print(f"   üéØ Pr√©cision globale: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"   üìä F1-Score moyen: {f1:.4f}")
        print(f"   ‚úÖ Pr√©dictions correctes: {resultats['predictions_correctes']}/{resultats['nb_echantillons_test']}")
        
        return resultats
    
    
    def creer_matrice_confusion(self, y_true: np.ndarray, y_pred: np.ndarray, 
                               noms_classes: List[str], nom_fichier: str = "matrice_confusion.png"):
        """
        Cr√©er et sauvegarder la matrice de confusion.
        
        Args:
            y_true (np.ndarray): Vraies √©tiquettes
            y_pred (np.ndarray): √âtiquettes pr√©dites
            noms_classes (List[str]): Noms des classes
            nom_fichier (str): Nom du fichier de sauvegarde
        """
        # Calculer la matrice de confusion
        cm = confusion_matrix(y_true, y_pred)
        
        # Cr√©er le graphique
        plt.figure(figsize=(10, 8))
        
        # Utiliser seaborn pour une matrice plus jolie
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=noms_classes, yticklabels=noms_classes,
                   square=True, linewidths=0.5)
        
        plt.title('Matrice de Confusion - FruitVision', fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Pr√©dictions du Mod√®le', fontsize=12)
        plt.ylabel('Vraies √âtiquettes', fontsize=12)
        
        # Rotation des labels pour meilleure lisibilit√©
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        
        # Sauvegarder
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        plt.savefig(chemin_complet, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"üìä Matrice de confusion sauvegard√©e: {chemin_complet}")
        
        # Analyser les erreurs communes
        self._analyser_erreurs_confusion(cm, noms_classes)
    
    
    def _analyser_erreurs_confusion(self, cm: np.ndarray, noms_classes: List[str]):
        """
        Analyser les erreurs les plus communes dans la matrice de confusion.
        
        Args:
            cm (np.ndarray): Matrice de confusion
            noms_classes (List[str]): Noms des classes
        """
        print("\nüîç Analyse des erreurs les plus fr√©quentes:")
        
        # Trouver les erreurs (√©l√©ments hors diagonale)
        erreurs = []
        for i in range(len(noms_classes)):
            for j in range(len(noms_classes)):
                if i != j and cm[i, j] > 0:  # Erreur (pas sur la diagonale)
                    erreurs.append((cm[i, j], noms_classes[i], noms_classes[j]))
        
        # Trier par nombre d'erreurs d√©croissant
        erreurs.sort(reverse=True)
        
        # Afficher les 3 erreurs les plus fr√©quentes
        print("   Top 3 des confusions:")
        for i, (nb_erreurs, vraie_classe, classe_predite) in enumerate(erreurs[:3]):
            print(f"   {i+1}. {vraie_classe} ‚Üí {classe_predite}: {nb_erreurs} erreurs")
        
        if not erreurs:
            print("   üéâ Aucune erreur d√©tect√©e! Mod√®le parfait!")
    
    
    def _sauvegarder_metriques(self, resultats: Dict, nom_fichier: str = "metriques_evaluation.json"):
        """
        Sauvegarder les m√©triques d'√©valuation.
        
        Args:
            resultats (Dict): R√©sultats de l'√©valuation
            nom_fichier (str): Nom du fichier
        """
        chemin_complet = os.path.join(self.dossier_resultats, nom_fichier)
        
        # Ajouter timestamp
        resultats['metadata'] = {
            'date_evaluation': datetime.now().isoformat(),
            'modele_utilise': 'FruitVision CNN'
        }
        
        with open(chemin_complet, 'w', encoding='utf-8') as f:
            json.dump(resultats, f, indent=2, ensure_ascii=False)
        
        print(f"üìä M√©triques sauvegard√©es: {chemin_complet}")


# Fonctions utilitaires ind√©pendantes
def calculer_temps_entrainement(debut: datetime, fin: datetime) -> str:
    """
    Calculer et formater le temps d'entra√Ænement.
    
    Args:
        debut (datetime): Heure de d√©but
        fin (datetime): Heure de fin
        
    Returns:
        str: Temps format√©
    """
    duree = fin - debut
    heures = duree.seconds // 3600
    minutes = (duree.seconds % 3600) // 60
    secondes = duree.seconds % 60
    
    if heures > 0:
        return f"{heures}h {minutes}m {secondes}s"
    elif minutes > 0:
        return f"{minutes}m {secondes}s"
    else:
        return f"{secondes}s"


def afficher_resume_modele(modele, noms_classes: List[str]):
    """
    Afficher un r√©sum√© du mod√®le avec des informations utiles.
    
    Args:
        modele: Mod√®le Keras
        noms_classes (List[str]): Noms des classes
    """
    print("üß† R√©sum√© du Mod√®le FruitVision")
    print("=" * 50)
    
    # Informations g√©n√©rales
    try:
        total_params = modele.count_params()
        print(f"üìä Param√®tres totaux: {total_params:,}")
        print(f"üíæ Taille estim√©e: ~{total_params * 4 / 1024 / 1024:.1f} MB")
    except:
        print("üìä Informations sur les param√®tres non disponibles")
    
    print(f"üçé Classes √† reconna√Ætre: {len(noms_classes)}")
    for i, nom in enumerate(noms_classes):
        print(f"   {i}: {nom}")
    
    print(f"üìê Forme d'entr√©e: {modele.input_shape}")
    print(f"üì§ Forme de sortie: {model.output_shape}")


# Test du module
if __name__ == "__main__":
    """
    Code de test pour les utilitaires d'entra√Ænement.
    """
    print("üß™ Test des Utilitaires d'Entra√Ænement")
    print("=" * 50)
    
    # Test du gestionnaire
    gestionnaire = GestionnaireEntrainement("test_results")
    
    # Test avec des donn√©es factices
    print("\nüî¨ Test avec donn√©es factices...")
    
    # Simuler un historique d'entra√Ænement
    historique_factice = {
        'accuracy': [0.6, 0.7, 0.8, 0.85, 0.9],
        'val_accuracy': [0.5, 0.6, 0.75, 0.8, 0.82],
        'loss': [1.5, 1.2, 0.8, 0.5, 0.3],
        'val_loss': [1.7, 1.3, 0.9, 0.6, 0.4]
    }
    
    gestionnaire.historique = historique_factice
    
    # Test des courbes d'apprentissage
    print("üìà Test des courbes d'apprentissage...")
    gestionnaire.tracer_courbes_apprentissage()
    
    # Test de la matrice de confusion avec donn√©es factices
    print("üìä Test de la matrice de confusion...")
    y_true_factice = np.array([0, 1, 2, 0, 1, 2, 0, 1])
    y_pred_factice = np.array([0, 1, 1, 0, 2, 2, 1, 1])
    noms_classes_test = ['Pomme', 'Banane', 'Kiwi']
    
    gestionnaire.creer_matrice_confusion(y_true_factice, y_pred_factice, noms_classes_test)
    
    print("\n‚úÖ Tous les tests des utilitaires pass√©s!")
    print("üìå Pr√™t pour l'entra√Ænement r√©el du mod√®le")