#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Rigoureux FruitVision V2
√âvaluation compl√®te pour v√©rifier si le biais "Pomme" est corrig√©
"""

import tensorflow as tf
import numpy as np
from PIL import Image
import os
import glob
import random
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import pandas as pd

class RigorousTestV2:
    def __init__(self, model_path=None):
        """Initialiser le testeur rigoureux"""
        self.classes = ['Pomme', 'Banane', 'Kiwi', 'Citron', 'Peche']
        self.model = None
        self.model_path = model_path
        
        if model_path:
            self.load_model(model_path)
    
    def load_model(self, model_path):
        """Charger le mod√®le V2"""
        try:
            self.model = tf.keras.models.load_model(model_path)
            print(f"‚úÖ Mod√®le V2 charg√©: {os.path.basename(model_path)}")
            return True
        except Exception as e:
            print(f"‚ùå Erreur chargement mod√®le: {e}")
            return False
    
    def find_latest_model(self):
        """Trouver le dernier mod√®le V2 sauvegard√©"""
        model_files = []
        
        # Chercher les mod√®les V2
        for pattern in ['fruivision_v2_best_*.h5', 'fruivision_v2_final_*.h5']:
            model_files.extend(glob.glob(f'models/{pattern}'))
        
        if not model_files:
            print("‚ùå Aucun mod√®le V2 trouv√©")
            return None
        
        # Prendre le plus r√©cent
        latest_model = sorted(model_files)[-1]
        print(f"üîç Mod√®le le plus r√©cent trouv√©: {latest_model}")
        
        return latest_model
    
    def preprocess_image(self, image_path):
        """Pr√©processer une image"""
        try:
            image = Image.open(image_path).convert('RGB')
            image = image.resize((100, 100))
            image_array = np.array(image) / 255.0
            image_array = np.expand_dims(image_array, axis=0)
            return image_array
        except Exception as e:
            print(f"‚ùå Erreur pr√©processing {image_path}: {e}")
            return None
    
    def test_dataset_official(self, num_per_class=20):
        """Test rigoureux sur le dataset officiel"""
        
        print("üß™ TEST RIGOUREUX SUR DATASET OFFICIEL")
        print("="*60)
        
        if self.model is None:
            print("‚ùå Aucun mod√®le charg√©")
            return None
        
        # Mapping des classes
        class_mappings = {
            'Pomme': ['Apple Red 1', 'Apple Red 2', 'Apple Red 3'],
            'Banane': ['Banana 1', 'Banana 3', 'Banana 4'],
            'Kiwi': ['Kiwi 1'],
            'Citron': ['Lemon 1', 'Lemon Meyer 1'],
            'Peche': ['Peach 1', 'Peach 2']
        }
        
        all_predictions = []
        all_true_labels = []
        all_confidences = []
        detailed_results = []
        
        base_dir = "data/fruits-360/test"
        
        print(f"üéØ Test de {num_per_class} images par classe")
        print("-" * 40)
        
        for our_class, dataset_folders in class_mappings.items():
            print(f"\nüçé Test classe: {our_class}")
            
            # Trouver le dossier disponible
            available_folder = None
            for folder in dataset_folders:
                folder_path = os.path.join(base_dir, folder)
                if os.path.exists(folder_path):
                    available_folder = folder
                    break
            
            if not available_folder:
                print(f"‚ùå Aucun dossier trouv√© pour {our_class}")
                continue
            
            # R√©cup√©rer les images
            folder_path = os.path.join(base_dir, available_folder)
            image_files = glob.glob(os.path.join(folder_path, "*.jpg"))
            
            if len(image_files) < num_per_class:
                print(f"‚ö†Ô∏è  Seulement {len(image_files)} images disponibles")
                test_images = image_files
            else:
                test_images = random.sample(image_files, num_per_class)
            
            print(f"üìÅ Dossier: {available_folder}")
            print(f"üìä Images test√©es: {len(test_images)}")
            
            class_correct = 0
            class_predictions = []
            
            for img_path in test_images:
                image_array = self.preprocess_image(img_path)
                if image_array is None:
                    continue
                
                # Pr√©diction
                pred = self.model.predict(image_array, verbose=0)
                pred_idx = np.argmax(pred[0])
                confidence = pred[0][pred_idx]
                predicted_class = self.classes[pred_idx]
                
                # Stocker les r√©sultats
                all_predictions.append(predicted_class)
                all_true_labels.append(our_class)
                all_confidences.append(confidence)
                class_predictions.append(predicted_class)
                
                is_correct = predicted_class == our_class
                if is_correct:
                    class_correct += 1
                
                detailed_results.append({
                    'image': os.path.basename(img_path),
                    'true_class': our_class,
                    'predicted_class': predicted_class,
                    'confidence': confidence,
                    'correct': is_correct
                })
            
            # Statistiques par classe
            if len(test_images) > 0:
                class_accuracy = class_correct / len(test_images)
                class_predictions_count = Counter(class_predictions)
                
                print(f"‚úÖ Accuracy: {class_accuracy:.1%} ({class_correct}/{len(test_images)})")
                print(f"üìà Pr√©dictions: {dict(class_predictions_count)}")
                
                # V√©rifier le biais "Pomme"
                pomme_predictions = class_predictions_count.get('Pomme', 0)
                pomme_rate = pomme_predictions / len(test_images)
                
                if our_class != 'Pomme' and pomme_rate > 0.5:
                    print(f"‚ö†Ô∏è  BIAIS D√âTECT√â: {pomme_rate:.1%} pr√©dit comme Pomme")
                elif our_class != 'Pomme' and pomme_rate < 0.2:
                    print(f"‚úÖ Biais r√©duit: seulement {pomme_rate:.1%} pr√©dit comme Pomme")
        
        return self.analyze_overall_results(all_predictions, all_true_labels, all_confidences, detailed_results)
    
    def analyze_overall_results(self, predictions, true_labels, confidences, detailed_results):
        """Analyser les r√©sultats globaux"""
        
        print("\n" + "="*60)
        print("üìä ANALYSE GLOBALE DES R√âSULTATS")
        print("="*60)
        
        if not predictions:
            print("‚ùå Aucune pr√©diction √† analyser")
            return None
        
        # Accuracy globale
        overall_accuracy = accuracy_score(true_labels, predictions)
        avg_confidence = np.mean(confidences)
        
        print(f"üéØ Accuracy globale: {overall_accuracy:.1%}")
        print(f"üéØ Confiance moyenne: {avg_confidence:.1%}")
        print(f"üéØ Total d'images: {len(predictions)}")
        
        # Analyse du biais "Pomme"
        prediction_counts = Counter(predictions)
        pomme_ratio = prediction_counts.get('Pomme', 0) / len(predictions)
        
        print(f"\nüîç ANALYSE DU BIAIS:")
        print(f"üìà Distribution des pr√©dictions:")
        for class_name in self.classes:
            count = prediction_counts.get(class_name, 0)
            percentage = count / len(predictions) * 100
            print(f"   {class_name:<8}: {count:3d} ({percentage:5.1f}%)")
        
        # Diagnostic du biais
        print(f"\nüéØ DIAGNOSTIC:")
        if pomme_ratio > 0.6:
            print(f"‚ùå BIAIS POMME S√âV√àRE: {pomme_ratio:.1%} des pr√©dictions")
            bias_status = "SEVERE"
        elif pomme_ratio > 0.4:
            print(f"‚ö†Ô∏è  BIAIS POMME MOD√âR√â: {pomme_ratio:.1%} des pr√©dictions")
            bias_status = "MODERATE"
        elif pomme_ratio > 0.25:
            print(f"‚úÖ BIAIS POMME L√âGER: {pomme_ratio:.1%} des pr√©dictions (acceptable)")
            bias_status = "LIGHT"
        else:
            print(f"‚úÖ AUCUN BIAIS POMME: {pomme_ratio:.1%} des pr√©dictions")
            bias_status = "NONE"
        
        # Matrice de confusion
        self.plot_confusion_matrix(true_labels, predictions)
        
        # Rapport d√©taill√© par classe
        print(f"\nüìã RAPPORT D√âTAILL√â PAR CLASSE:")
        report = classification_report(true_labels, predictions, 
                                     target_names=self.classes, 
                                     output_dict=True)
        
        for class_name in self.classes:
            if class_name in report:
                precision = report[class_name]['precision']
                recall = report[class_name]['recall']
                f1 = report[class_name]['f1-score']
                support = report[class_name]['support']
                
                print(f"   {class_name:<8}: P={precision:.3f} R={recall:.3f} F1={f1:.3f} (n={support})")
        
        # Comparaison avec V1
        print(f"\nüìà COMPARAISON AVEC V1:")
        print(f"   V1: 40% accuracy, biais Pomme s√©v√®re")
        print(f"   V2: {overall_accuracy:.1%} accuracy, biais {bias_status}")
        
        if overall_accuracy > 0.8 and bias_status in ["LIGHT", "NONE"]:
            print(f"üéâ AM√âLIORATION SIGNIFICATIVE!")
        elif overall_accuracy > 0.6:
            print(f"‚úÖ Am√©lioration mod√©r√©e")
        else:
            print(f"‚ùå Am√©lioration insuffisante")
        
        return {
            'accuracy': overall_accuracy,
            'avg_confidence': avg_confidence,
            'bias_status': bias_status,
            'pomme_ratio': pomme_ratio,
            'predictions_distribution': prediction_counts,
            'detailed_results': detailed_results
        }
    
    def plot_confusion_matrix(self, true_labels, predictions):
        """Cr√©er la matrice de confusion"""
        
        cm = confusion_matrix(true_labels, predictions, labels=self.classes)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=self.classes, yticklabels=self.classes)
        plt.title('Matrice de Confusion - FruitVision V2', fontsize=16)
        plt.xlabel('Pr√©dictions', fontsize=12)
        plt.ylabel('Vraies Classes', fontsize=12)
        
        # Sauvegarder
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plt.savefig(f'plots/confusion_matrix_test_v2_{timestamp}.png', 
                   dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"üìä Matrice de confusion sauvegard√©e dans plots/")
    
    def test_synthetic_images(self):
        """Test sur images synth√©tiques pour v√©rifier la robustesse"""
        
        print("\nüé® TEST SUR IMAGES SYNTH√âTIQUES")
        print("="*40)
        
        if self.model is None:
            print("‚ùå Aucun mod√®le charg√©")
            return
        
        # Cr√©er des images color√©es simples
        synthetic_tests = {
            'Rouge (Pomme)': (255, 0, 0),
            'Jaune (Banane)': (255, 255, 0),
            'Vert (Kiwi)': (0, 255, 0),
            'Jaune (Citron)': (255, 255, 100),
            'Orange (P√™che)': (255, 165, 0)
        }
        
        print("üîç Test de g√©n√©ralisation sur couleurs pures:")
        
        for description, color in synthetic_tests.items():
            # Cr√©er une image color√©e
            image = Image.new('RGB', (100, 100), color)
            image_array = np.array(image) / 255.0
            image_array = np.expand_dims(image_array, axis=0)
            
            # Pr√©diction
            pred = self.model.predict(image_array, verbose=0)
            pred_idx = np.argmax(pred[0])
            confidence = pred[0][pred_idx]
            predicted_class = self.classes[pred_idx]
            
            print(f"   {description:<18} ‚Üí {predicted_class:<8} ({confidence:.1%})")
        
        print("\nüí° Note: Les images synth√©tiques testent la robustesse du mod√®le")
        print("   Un bon mod√®le ne devrait pas avoir de confiance excessive sur ces images")

def main():
    """Fonction principale de test rigoureux"""
    
    print("üî¨ FRUIVISION V2 - TEST RIGOUREUX")
    print("="*70)
    print("üéØ Objectif: V√©rifier si le biais 'Pomme' a √©t√© corrig√©")
    print("üìã Tests: Dataset officiel + Images synth√©tiques")
    print("")
    
    # Cr√©er le testeur
    tester = RigorousTestV2()
    
    # Trouver le mod√®le le plus r√©cent
    latest_model = tester.find_latest_model()
    
    if latest_model is None:
        print("‚ùå Aucun mod√®le V2 trouv√© pour les tests")
        return
    
    # Charger le mod√®le
    if not tester.load_model(latest_model):
        return
    
    # Test principal sur dataset officiel
    results = tester.test_dataset_official(num_per_class=15)
    
    # Test sur images synth√©tiques
    tester.test_synthetic_images()
    
    # R√©sum√© final
    if results:
        print("\n" + "="*70)
        print("üéØ R√âSUM√â FINAL")
        print("="*70)
        print(f"‚úÖ Accuracy: {results['accuracy']:.1%}")
        print(f"‚úÖ Confiance moyenne: {results['avg_confidence']:.1%}")
        print(f"‚úÖ Statut biais: {results['bias_status']}")
        
        if results['accuracy'] > 0.8 and results['bias_status'] in ["LIGHT", "NONE"]:
            print("üéâ SUCC√àS: Le mod√®le V2 est significativement am√©lior√©!")
        else:
            print("‚ö†Ô∏è  Le mod√®le V2 n√©cessite encore des am√©liorations")

if __name__ == "__main__":
    main()